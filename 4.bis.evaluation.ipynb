{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ab9b588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.evaluation as eval_utils\n",
    "import utils.model as model_utils\n",
    "import torch\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils.embed_concepts as embed_concepts\n",
    "import tqdm.auto as tqdm\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46ed1087",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_CONFIG = {\n",
    "    \"path_all_concept\": \"D:/lora_finetune_eval/basic_info/concept_all.parquet\",\n",
    "    \"path_is_a\" : \"D:/lora_finetune_eval/basic_info/graph_is_a_invariant.csv\",\n",
    "    \"mapped_concept\": \"D:/lora_finetune_eval/basic_info/mapped_concepts_2025-04-01.csv\",\n",
    "    \"training_triplet_idx\": \"D:/lora_finetune_eval/basic_info/training_anchor_idx_1M.parquet\",\n",
    "\n",
    "    \"icd_snomed\" : \"D:/lora_finetune_eval/icd_snomed/\",\n",
    "\n",
    "    \"embedding_save_path\": \"D:/lora_finetune_eval/embedding_by_model/\",\n",
    "    \"syn_embedding_save_path\": \"D:/lora_finetune_eval/syn_embedding_by_model/\",\n",
    "    \"new_exp_embedding_save_path\": \"D:/lora_finetune_eval/new_exp_embedding_by_model/\",\n",
    "    \"embedding_icd_snomed_save_path\": \"D:/lora_finetune_eval/embedding_icd_snomed_by_model/\",\n",
    "\n",
    "    \"result_save_path\": \"D:/lora_finetune_eval/result_by_model/\",\n",
    "\n",
    "}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ac6e67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = embed_concepts.MODEL_CONFIG.keys()\n",
    "\n",
    "df_concept_all = pl.read_parquet(FILE_CONFIG[\"path_all_concept\"])\n",
    "df_concept_all_idx = set(df_concept_all[\"idx\"].unique().to_list())\n",
    "\n",
    "df_concept_train = pl.read_parquet(FILE_CONFIG[\"training_triplet_idx\"])\n",
    "df_concept_train_idx = set(df_concept_train[\"idx\"].unique().to_list())\n",
    "\n",
    "df_concept_test_idx = df_concept_all_idx - df_concept_train_idx\n",
    "df_concept_test = list(df_concept_test_idx)\n",
    "\n",
    "id2idx = dict(zip(df_concept_all[\"id\"], df_concept_all[\"idx\"]))\n",
    "\n",
    "df_concept_test_fd = df_concept_all.filter(pl.col(\"idx\").is_in(df_concept_test)).filter(pl.col(\"status\") == \"defined\")[\"idx\"].unique().to_list()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b324072b",
   "metadata": {},
   "source": [
    "# task 1: retrieve label given expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc688b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrrs_1 = []\n",
    "models_1 = []\n",
    "ranks_1 = {}\n",
    "for model_key in model_names:\n",
    "\n",
    "    if os.path.exists(FILE_CONFIG[\"embedding_save_path\"] + model_key + \".pt\"):\n",
    "        print(f\"Evaluating {model_key}...\")\n",
    "        embeddings = torch.load(FILE_CONFIG[\"embedding_save_path\"] + model_key + \".pt\")\n",
    "        embedding_exp= embeddings[\"expressions_embeddings\"]\n",
    "        embedding_label = embeddings[\"labels_embeddings\"]\n",
    "        rank = eval_utils.top_k_array_by_batch(df_concept_test_fd, embedding_exp, embedding_label,device, 100)\n",
    "        mrr_rank = eval_utils.compute_mmr(rank)\n",
    "        mrrs_1.append(mrr_rank)\n",
    "        models_1.append(model_key)\n",
    "        ranks_1[model_key] = rank\n",
    "        print(f\"MRR: {mrr_rank}\")\n",
    "\n",
    "    else: \n",
    "        continue\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca0948c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.DataFrame({\"model\": models_1, \"mrr_1\": mrrs_1}).write_csv(FILE_CONFIG[\"result_save_path\"] + \"mrr_1.csv\")\n",
    "with open(FILE_CONFIG[\"result_save_path\"] + \"ranks_1.pkl\", 'wb') as f:\n",
    "    pickle.dump(ranks_1, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96579c79",
   "metadata": {},
   "source": [
    "# task 2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5018493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating finetune_1M...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yy\\Desktop\\quantizer\\vector-quantize-pytorch\\utils\\evaluation.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_matrix = torch.tensor(query_matrix, dtype=torch.float32).to(device)\n",
      "c:\\Users\\yy\\Desktop\\quantizer\\vector-quantize-pytorch\\utils\\evaluation.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  candidate_matrix = torch.tensor(candidate_matrix, dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 183000\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x768 and 384x392641)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m embedding_syn_test_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(embedding_syn_idx)\u001b[38;5;241m.\u001b[39mintersection(\u001b[38;5;28mset\u001b[39m(df_concept_test_fd)))\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Compute rankings\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m rank_w_exp \u001b[38;5;241m=\u001b[39m \u001b[43meval_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_k_array_syn\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_syn_test_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_syn_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_syn_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_exp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m rank_w_label \u001b[38;5;241m=\u001b[39m eval_utils\u001b[38;5;241m.\u001b[39mtop_k_array_syn(embedding_syn_test_idx, embedding_syn_idx, embedding_syn_matrix, embedding_label, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     28\u001b[0m mrr_rank_exp \u001b[38;5;241m=\u001b[39m eval_utils\u001b[38;5;241m.\u001b[39mcompute_mmr(rank_w_exp)\n",
      "File \u001b[1;32mc:\\Users\\yy\\Desktop\\quantizer\\vector-quantize-pytorch\\utils\\evaluation.py:106\u001b[0m, in \u001b[0;36mtop_k_array_syn\u001b[1;34m(idx_syns_test, idx_syns, query_matrix, candidate_matrix, batch_size, device)\u001b[0m\n\u001b[0;32m    103\u001b[0m query \u001b[38;5;241m=\u001b[39m query_matrix[i]  \u001b[38;5;66;03m# (batch_size, dim)\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Compute scores for the batch\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidate_matrix_T\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch_size, num_candidates)\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Sort each query's candidates\u001b[39;00m\n\u001b[0;32m    109\u001b[0m sorted_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margsort(scores, descending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x768 and 384x392641)"
     ]
    }
   ],
   "source": [
    "mrrs_2_exp = []\n",
    "mrrs_2_label = []\n",
    "\n",
    "models_2 = []\n",
    "ranks_2 = {}\n",
    "for model_key in model_names:\n",
    "\n",
    "    if  os.path.exists(FILE_CONFIG[\"syn_embedding_save_path\"] + model_key + \".pt\"):\n",
    "        print(f\"Evaluating {model_key}...\")\n",
    "\n",
    "        syn_embeddings = torch.load(FILE_CONFIG[\"syn_embedding_save_path\"] + model_key + \".pt\")\n",
    "        embeddings = torch.load(FILE_CONFIG[\"embedding_save_path\"] + model_key + \".pt\")\n",
    "\n",
    "        embedding_exp = embeddings[\"expressions_embeddings\"]\n",
    "        embedding_label = embeddings[\"labels_embeddings\"]\n",
    "\n",
    "\n",
    "        embedding_syn_idx = syn_embeddings[\"idx\"]\n",
    "        embedding_syn_matrix = syn_embeddings[\"expressions_embeddings\"]\n",
    "\n",
    "        # Determine test indices\n",
    "        embedding_syn_test_idx = list(set(embedding_syn_idx).intersection(set(df_concept_test_fd)))\n",
    "\n",
    "        # Compute rankings\n",
    "        rank_w_exp = eval_utils.top_k_array_syn(embedding_syn_test_idx, embedding_syn_idx, embedding_syn_matrix, embedding_exp, device=device)\n",
    "        rank_w_label = eval_utils.top_k_array_syn(embedding_syn_test_idx, embedding_syn_idx, embedding_syn_matrix, embedding_label, device=device)\n",
    "        \n",
    "        mrr_rank_exp = eval_utils.compute_mmr(rank_w_exp)\n",
    "        mrr_rank_label = eval_utils.compute_mmr(rank_w_label)\n",
    "\n",
    "\n",
    "        mrrs_2_exp.append(mrr_rank_exp)\n",
    "        mrrs_2_label.append(mrr_rank_label)\n",
    "\n",
    "        models_2.append(model_key)\n",
    "\n",
    "        ranks_2[model_key]['expession'] = rank_w_exp\n",
    "        ranks_2[model_key]['label'] = rank_w_label\n",
    "\n",
    "\n",
    "        print(f\"MRR: {mrr_rank_exp}\")\n",
    "        print(f\"MRR: {mrr_rank_label}\")\n",
    "\n",
    "    else: \n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0227eb2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrrs_2_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48996d86",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
