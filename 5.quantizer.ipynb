{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2db09741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import polars as pl\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from vector_quantize_pytorch import ResidualVQ\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from utils.file_config import FILE_CONFIG as fc\n",
    "import utils.evaluation as eval_utils\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "\n",
    "# === Config ===\n",
    "class Config:\n",
    "    alpha = 0\n",
    "    num_epochs = 100\n",
    "    batch_size = 2048*32\n",
    "    lr = 3e-4\n",
    "    dim = 768\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    seed = 1234\n",
    "    codebook_config =[\n",
    "    {'num_codebooks': 5, 'codebook_size': 512, 'use_cosine': True, 'model_name': 'rvq_5x512_cos'},\n",
    "    {'num_codebooks': 6, 'codebook_size': 512, 'use_cosine': True, 'model_name': 'rvq_6x512_cos'},\n",
    "    {'num_codebooks': 7, 'codebook_size': 512, 'use_cosine': True, 'model_name': 'rvq_7x512_cos'},\n",
    "    {'num_codebooks': 5, 'codebook_size': 1024, 'use_cosine': True, 'model_name': 'rvq_5x1024_cos'},\n",
    "    {'num_codebooks': 6, 'codebook_size': 1024, 'use_cosine': True, 'model_name': 'rvq_6x1024_cos'},\n",
    "    {'num_codebooks': 7, 'codebook_size': 1024, 'use_cosine': True, 'model_name': 'rvq_7x1024_cos'},\n",
    "\n",
    "    {'num_codebooks': 5, 'codebook_size': 512, 'use_cosine': False, 'model_name': 'rvq_5x512_l2'},\n",
    "    {'num_codebooks': 6, 'codebook_size': 512, 'use_cosine': False, 'model_name': 'rvq_6x512_l2'},\n",
    "    {'num_codebooks': 7, 'codebook_size': 512, 'use_cosine': False, 'model_name': 'rvq_7x512_l2'},\n",
    "    {'num_codebooks': 5, 'codebook_size': 1024, 'use_cosine': False, 'model_name': 'rvq_5x1024_l2'},\n",
    "    {'num_codebooks': 6, 'codebook_size': 1024, 'use_cosine': False, 'model_name': 'rvq_6x1024_l2'},\n",
    "    {'num_codebooks': 7, 'codebook_size': 1024, 'use_cosine': False, 'model_name': 'rvq_7x1024_l2'},\n",
    "]\n",
    "\n",
    "torch.manual_seed(Config.seed)\n",
    "\n",
    "\n",
    "def load_embeddings(file_config, model, config):\n",
    "    df_concept_all = pl.read_parquet(file_config[\"path_all_concept\"])\n",
    "    df_mapped = pl.read_csv(file_config[\"mapped_concept\"])\n",
    "    idx_mapped = df_mapped.join(df_concept_all, left_on=\"n.id\", right_on=\"id\")[\"idx\"].unique().to_list()\n",
    "    embedding_path = file_config[\"embedding_save_path\"] + f\"/{model}.pt\"\n",
    "    full_embeddings_l = torch.load(embedding_path)[\"labels_embeddings\"].to(config.device)\n",
    "    full_embeddings_exp = torch.load(embedding_path)[\"expressions_embeddings\"].to(config.device)\n",
    "    mapped_embeddings = full_embeddings_l[idx_mapped, :]\n",
    "\n",
    "    return full_embeddings_l, full_embeddings_exp, mapped_embeddings\n",
    "\n",
    "\n",
    "def train(model, train_loader, config):\n",
    "    \n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=config.lr)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        print(f\"\\n=== Epoch {epoch + 1}/{config.num_epochs} ===\")\n",
    "        epoch_losses = []\n",
    "        for x_batch in tqdm(train_loader):\n",
    "\n",
    "            x = x_batch[0].to(config.device)\n",
    "            opt.zero_grad()\n",
    "\n",
    "            quantized, _, _ = model(x)\n",
    "            # out = out.clamp(-1., 1.)\n",
    "            if config.use_cosine:\n",
    "                loss = 1 - F.cosine_similarity(x, quantized, dim=-1).mean()\n",
    "            else:\n",
    "                loss = F.mse_loss(x, quantized)\n",
    "\n",
    "            epoch_losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        \n",
    "        avg_epoch_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "        print(f\"Epoch {epoch+1} | Avg batch loss: {avg_epoch_loss:.4f}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate(model, embeddings):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embeddings = embeddings.to(Config.device)\n",
    "        quantized, indices, _ = model(embeddings)\n",
    "        cos_sim = F.cosine_similarity(embeddings, quantized, dim=-1).mean()\n",
    "        print(f\"Average cosine similarity: {cos_sim.item():.4f}\")\n",
    "        return quantized, indices\n",
    "    \n",
    "def get_dataloader(embeddings,config):\n",
    "    dataset = TensorDataset(embeddings)\n",
    "    return DataLoader(dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "def train_all_rvq_configs(cfg_base, train_loader,embeddings_mapped):\n",
    "    results = []\n",
    "    for config in cfg_base.codebook_config:\n",
    "        cfg = deepcopy(cfg_base)\n",
    "        cfg.num_codebooks = config['num_codebooks']\n",
    "        cfg.codebook_size = config['codebook_size']\n",
    "        cfg.use_cosine = config['use_cosine']\n",
    "        config_id = f\"{cfg.num_codebooks}x{cfg.codebook_size}_{'cos' if cfg.use_cosine else 'l2'}\"\n",
    "\n",
    "        print(f\"\\n=== Training {config_id} ===\")\n",
    "\n",
    "        model = ResidualVQ(\n",
    "            dim=cfg.dim,\n",
    "            num_quantizers=cfg.num_codebooks,\n",
    "            codebook_size=cfg.codebook_size,\n",
    "            \n",
    "            learnable_codebook=True,\n",
    "            ema_update=False,\n",
    "            kmeans_init=True,\n",
    "            kmeans_iters=10,\n",
    "            use_cosine_sim=cfg.use_cosine\n",
    "        ).to(cfg.device)\n",
    "\n",
    "        model = train(model, train_loader, cfg)\n",
    "\n",
    "        torch.save(model.state_dict(), os.path.join(fc[\"model_q_save_path\"], f\"rvq_{config_id}.pt\"))\n",
    "        quantized, _ = evaluate(model, embeddings_mapped)\n",
    "\n",
    "        print(f\"Model {config_id} trained and saved.\")\n",
    "\n",
    "        results.append({\n",
    "        'config_id': config_id,\n",
    "        'num_codebooks': cfg.num_codebooks,\n",
    "        'codebook_size': cfg.codebook_size,\n",
    "        'use_cosine': cfg.use_cosine,\n",
    "        'cos_sim_mapped': F.cosine_similarity(embeddings_mapped, quantized, dim=-1).mean().item()\n",
    "    })\n",
    "\n",
    "def load_model(cfg_base, config_dict):\n",
    "    # Merge config base with specific codebook setting\n",
    "    cfg = deepcopy(cfg_base)\n",
    "    cfg.num_codebooks = config_dict['num_codebooks']\n",
    "    cfg.codebook_size = config_dict['codebook_size']\n",
    "    cfg.use_cosine = config_dict['use_cosine']\n",
    "    \n",
    "    config_id = config_dict['model_name']  # already follows \"rvq_XxY_cos/l2\"\n",
    "\n",
    "    model = ResidualVQ(\n",
    "        dim=cfg.dim,\n",
    "        num_quantizers=cfg.num_codebooks,\n",
    "        codebook_size=cfg.codebook_size,\n",
    "        learnable_codebook=True,\n",
    "        ema_update=False,\n",
    "        kmeans_init=True,\n",
    "        kmeans_iters=10,\n",
    "        use_cosine_sim=cfg.use_cosine\n",
    "    ).to(cfg.device)\n",
    "\n",
    "    model_path = os.path.join(fc[\"model_q_save_path\"], f\"{config_id}.pt\")\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found at {model_path}\")\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(f\"\\n=== Model {config_id} loaded ===\")\n",
    "\n",
    "    return model, config_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c84a33e",
   "metadata": {},
   "source": [
    "# train all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a930725",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config()\n",
    "\n",
    "# === Dataset & DataLoader ===\n",
    "full_embeddings_l,full_embeddings_exp, mapped_embeddings = load_embeddings(file_config=fc, model= \"sapbert_lora_triplet16\", config=cfg)\n",
    "train_loader = get_dataloader(torch.concat((full_embeddings_l, full_embeddings_exp)), cfg)\n",
    "\n",
    "# === Train Residual VQ for all configurations ===\n",
    "result = train_all_rvq_configs(cfg, train_loader, mapped_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4f71b7",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c4da650",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concept_all = pl.read_parquet(fc[\"path_all_concept\"])\n",
    "df_concept_all_idx = set(df_concept_all[\"idx\"].unique().to_list())\n",
    "\n",
    "df_concept_train = pl.read_parquet(fc[\"training_triplet_idx\"])\n",
    "df_concept_train_idx = set(df_concept_train[\"idx\"].unique().to_list())\n",
    "\n",
    "df_concept_test_idx = df_concept_all_idx - df_concept_train_idx\n",
    "df_concept_test = list(df_concept_test_idx)\n",
    "\n",
    "id2idx = dict(zip(df_concept_all[\"id\"], df_concept_all[\"idx\"]))\n",
    "\n",
    "df_concept_test_fd = df_concept_all.filter(pl.col(\"idx\").is_in(df_concept_test)).filter(pl.col(\"status\") == \"defined\")[\"idx\"].unique().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55ec5a15",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_model() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m cfg \u001b[38;5;241m=\u001b[39m Config()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mcodebook_config:\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: load_model() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "cfg = Config()\n",
    "for config in cfg.codebook_config:\n",
    "    \n",
    "    load_model(cfg, config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ae9e24",
   "metadata": {},
   "source": [
    "# eval task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c112fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrrs_1 = []\n",
    "models_1 = []\n",
    "ranks_1 = {}\n",
    "\n",
    "embeddings = torch.load(fc[\"embedding_save_path\"] + f\"/sapbert_lora_triplet16.pt\")\n",
    "embedding_exp = embeddings[\"expressions_embeddings\"]\n",
    "embedding_label = embeddings[\"labels_embeddings\"]\n",
    "embedding_exp_q,_ = evaluate(rvq_vanilla, embedding_exp)\n",
    "embedding_label_q,_ = evaluate(rvq_vanilla, embedding_label)\n",
    "rank = eval_utils.top_k_array_by_batch(df_concept_test_fd, embedding_exp_q, embedding_label_q,cfg.device, 100)\n",
    "mrr_rank = eval_utils.compute_mmr(rank)\n",
    "mrrs_1.append(mrr_rank)\n",
    "models_1.append(\"vanilla_4_512\")\n",
    "ranks_1[\"vanilla_4_512\"] = rank\n",
    "print(f\"MRR: {mrr_rank}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a91917",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfg.use_cosine = True  # Set to True if you want to use cosine similarity\n",
    "rvq_cosine = train(rvq_cosine, train_loader, cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7706a7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = torch.load(fc[\"embedding_save_path\"] + f\"/sapbert_lora_triplet16.pt\")\n",
    "embedding_exp = embeddings[\"expressions_embeddings\"]\n",
    "embedding_label = embeddings[\"labels_embeddings\"]\n",
    "embedding_exp_q,_ = evaluate(rvq_cosine, embedding_exp)\n",
    "embedding_label_q,_ = evaluate(rvq_cosine, embedding_label)\n",
    "rank = eval_utils.top_k_array_by_batch(df_concept_test_fd, embedding_exp_q, embedding_label_q,cfg.device, 100)\n",
    "mrr_rank = eval_utils.compute_mmr(rank)\n",
    "mrrs_1.append(mrr_rank)\n",
    "models_1.append(\"cosin_4_512\")\n",
    "ranks_1[\"cosin_4_512\"] = rank\n",
    "print(f\"MRR: {mrr_rank}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
